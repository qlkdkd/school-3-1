# 2. 딥러닝을 위한 기초 지식
## 02. 미분을 알아보자
### 극한의 의미
* 수학에서의 극한: 어떤 값이 특정한 값에 무한히 가까워질 때, 그 값을 변수로 하는 함수가 도달하려는 '목표점'을 의미
* 함수 $f(x)$가 있고, 이 함수의 변수는 $x!=a$이면서 $a$에 한없이 가까워질 때, 특정한 값 $L$에 한없이 가까워지면 $f(x)$는 $L$에 수렴한다고 함

![image](https://github.com/qlkdkd/univ-3-1/assets/71871927/549ea072-bbe0-4436-92f4-8442c4c575b0)

* 미적분학은 변화하는 양을 정확하게 기술하는 수학의 분야로, 물리학, 공학, 경제학 등 다양한 분야에서 응용됨.
* 미분은 딥러닝 알고리즘의 최적값을 찾는데 사용됨.

### 순간변화율=미분
* 미분: 함수의 순간변화율. 다음과 같이 독립변수 $x$의 변화량에 대한 함수 $f(x)$의 변화량의 비율의 극한으로 정의됨.

![image](https://github.com/qlkdkd/univ-3-1/assets/71871927/0e2f6481-4cba-42b7-8a8a-bfd65d3fabc1)

* 함수 $f'(x)$를 $f(x)$의 도함수라고도 함. 도함수는 $\frac{d}{d(x)}f(x)$라고도 함.

![image](https://github.com/qlkdkd/univ-3-1/assets/71871927/b64e0303-fc39-454d-95ea-31a9d542c758)
* $\delta x$가 그림 1에서 그림2, 그림3으로 갈수록 점점 작아지며 이 값이 0에 가까워지면 그림 3과 같이 기울기값도 변함.
* 그림 3을 살펴보면 그림 속의 직선은 (a, f(a))점에서 이 점을 지나는 접선의 기울기와 같아지는 것을 볼 수 있음

* $f(x)=x^2$함수가 있을 때 이 함수의 미분값은 다음과 같이 구함.

![image](https://github.com/qlkdkd/univ-3-1/assets/71871927/04968922-d48a-4b34-b42b-baddd20e3e14)

위와 같은 식을 통해서 유도한 여러가지 함수의 미분값을 구하는 기본 공식

![image](https://github.com/qlkdkd/univ-3-1/assets/71871927/099343d9-00c2-4ef2-bc6f-0355700e57bd)

### 합성함수와 연쇄법칙
* 연쇄법칙: 어떤 함수 y를 x에 대해 미분할 때, 매개변수 t를 두어 다음과 같이 미분함

![image](https://github.com/qlkdkd/univ-3-1/assets/71871927/98fd7a46-3293-4ac0-ae90-7f0dd9c20e14)

* $y=f(g(x))$와 같은 형태의 함수를 미분하는 방법

![image](https://github.com/qlkdkd/univ-3-1/assets/71871927/30196a6a-4274-4c73-9344-56442eecd18b)

### 딥러닝에서 목적 함수(비용함수)의 미분이 중요한 이유
* 머신러닝과 딥러닝에서는 데이터의 복잡한 패턴을 스스로 찾아가도록 학습 알고리즘이 필요한데 이 알고리즘을 모델이라고 함.

![image](https://github.com/qlkdkd/univ-3-1/assets/71871927/f3bc5802-0c59-4cb5-9283-acc11b434132)

### 여러개의 변수를 가진 함수의 최적화를 위한 편미분
* 목적함수: 모델이 최대화하거나 최소화하려는 함수
* 평균변화율: 위 그림의 가운데에 있는 분홍색 삼각형을 보면 변수 $x$가 $\Delta x$만큼 변할 때, 목적함수는 $\Delta y$만큼 변한다.
* 미분과 접선의 기울기: $x=x_2$인 지점에서 이 삼각형을 매우 작게 만들었다. 이 삼각형을 무한히 작게 만들면 $x_2$지점에서의 목적함수에 대한 미분값을 얻을 수 있다.
* 경사하강: 접선의 기울기를 따라 반대로 내려오면 해당 지점에서 목적 함수의 값이 더 작은 쪽으로 이동할 수 있다. $x_2$의 위치에서 $x$가 증가하면 $y$도 증가하므로 접선의 기울기는 양수이다. 따라서 $x_2$에서 음의 방향으로 움직이면 목적 함수를 줄일 수 있다. 즉 $f'(x_2)$의 반대방향인 $-f'(x_2)$로 이동한다. 이것이 경사하강법이라고 한다.
* 최적화: 최적화는 모델이 학습 데이터에 대한 오차를 줄이면서 가장 잘 예측할 수 있도록 가중치와 편향값과 같은 파라미터를 조절하는 것을 말한다.
